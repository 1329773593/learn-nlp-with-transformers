{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "本文涉及的jupter notebook在[篇章4代码库中](https://github.com/datawhalechina/learn-nlp-with-transformers/tree/main/docs/%E7%AF%87%E7%AB%A04-%E4%BD%BF%E7%94%A8Transformers%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1)。\n",
        "\n",
        "建议直接使用google colab notebook打开本教程，可以快速下载相关数据集和模型。\n",
        "如果您正在google的colab中打开这个notebook，您可能需要安装Transformers和🤗Datasets库。将以下命令取消注释即可安装。"
      ],
      "metadata": {
        "id": "bTAeNLV3WdB0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# ! pip install datasets transformers \n",
        "# -i https://pypi.tuna.tsinghua.edu.cn/simple"
      ],
      "outputs": [],
      "metadata": {
        "id": "bAWdZQdTWdB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "如果您是在本地机器上打开这个jupyter笔记本，请确保您的环境安装了上述库的最新版本。\n",
        "\n",
        "您可以在[这里](https://github.com/huggingface/transformers/tree/master/examples/language-modeling)找到这个jupyter笔记本的具体的python脚本文件，还可以通过分布式的方式使用多个gpu或tpu来微调您的模型。"
      ],
      "metadata": {
        "id": "7FC_ZTXsWdB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 微调语言模型"
      ],
      "metadata": {
        "id": "BgQvrzh3WdB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在当前jupyter笔记本中，我们将说明如何使用语言模型任务微调任意[🤗Transformers](https://github.com/huggingface/transformers) 模型。 \n",
        "\n",
        "本教程将涵盖两种类型的语言建模任务:\n",
        "\n",
        "+ 因果语言模型（Causal language modeling，CLM）：模型需要预测句子中的下一位置处的字符（类似BERT类模型的decoder和GPT，从左往右输入字符）。为了确保模型不作弊，模型会使用一个注意掩码防止模型看到之后的字符。例如，当模型试图预测句子中的i+1位置处的字符时，这个掩码将阻止它访问i位置之后的字符。\n",
        "\n",
        "![推理表示因果语言建模任务图片](./images/causal_language_modeling.png)\n",
        "\n",
        "+ 掩蔽语言建模（Masked language modeling，MLM）：模型需要恢复输入中被\"MASK\"掉的一些字符（BERT类模型的预训练任务）。这种方式模型可以看到整个句子，因此模型可以根据“\\[MASK\\]”标记之前和之后的字符来预测该位置被“\\[MASK\\]”之前的字符。\n",
        "\n",
        "![Widget inference representing the masked language modeling task](images/masked_language_modeling.png)\n",
        "\n",
        "接下来，我们将说明如何轻松地为每个任务加载和预处理数据集，以及如何使用“Trainer”API对模型进行微调。\n",
        "\n",
        "当然您也可以直接在分布式环境或TPU上运行该jupyter笔记本的python脚本版本，可以在[examples文件夹](https://github.com/huggingface/transformers/tree/master/examples)中找到。"
      ],
      "metadata": {
        "id": "hqHxDcitWdB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 准备数据"
      ],
      "metadata": {
        "id": "GobLIFiRWdB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在接下来的这些任务中，我们将使用[Wikitext 2](https://huggingface.co/datasets/wikitext#data-instances)数据集作为示例。您可以通过🤗Datasets库加载该数据集："
      ],
      "metadata": {
        "id": "Tfkh562BWdB5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "from datasets import load_dataset\n",
        "datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 8.33kB [00:00, 1.49MB/s]                   \n",
            "Downloading: 5.83kB [00:00, 1.77MB/s]                   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset wikitext/wikitext-2-raw-v1 (download: 4.50 MiB, generated: 12.91 MiB, post-processed: Unknown size, total: 17.41 MiB) to /Users/niepig/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|██████████| 4.72M/4.72M [00:02<00:00, 1.91MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset wikitext downloaded and prepared to /Users/niepig/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20. Subsequent calls will reuse this data.\n"
          ]
        }
      ],
      "metadata": {
        "id": "vtDCQuMSWdB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "如果碰到以下错误：\n",
        "![request Error](images/request_error.png)\n",
        "\n",
        "解决方案:\n",
        "\n",
        "MAC用户: 在 ```/etc/hosts``` 文件中添加一行 ```199.232.68.133  raw.githubusercontent.com```\n",
        "\n",
        "Windowso用户: 在 ```C:\\Windows\\System32\\drivers\\etc\\hosts```  文件中添加一行 ```199.232.68.133  raw.githubusercontent.com```"
      ],
      "metadata": {
        "id": "IqMUxoJrWdB7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "当然您也可以用公开在[hub](https://huggingface.co/datasets)上的任何数据集替换上面的数据集，或者使用您自己的文件。只需取消注释以下单元格，并将路径替换为将导致您的文件路径："
      ],
      "metadata": {
        "id": "Wjl5FpYDWdB7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# datasets = load_dataset(\"text\", data_files={\"train\": path_to_train.txt, \"validation\": path_to_validation.txt}"
      ],
      "outputs": [],
      "metadata": {
        "id": "fncDchlaWdB7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "您还可以从csv或JSON文件加载数据集，更多信息请参阅[完整文档](https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files)。\n",
        "\n",
        "要访问一个数据中实际的元素，您需要先选择一个key，然后给出一个索引:"
      ],
      "metadata": {
        "id": "ODIsscsTWdB8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "datasets[\"train\"][10]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': ' The game \\'s battle system , the BliTZ system , is carried over directly from Valkyira Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters \\' turns . Each character has a field and distance of movement limited by their Action Gauge . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific \" Potentials \" , skills unique to each character . They are divided into \" Personal Potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" Battle Potentials \" , which are grown throughout the game and always grant boons to a character . To learn Battle Potentials , each character has a unique \" Masters Table \" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special Abilities that grant them temporary boosts on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without depleting his Action Point gauge , the character Reila can shift into her \" Valkyria Form \" and become invincible , while Imca can target multiple enemy units with her heavy weapon . \\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H887z9CbWdB8",
        "outputId": "f9d05402-f99b-40da-b672-887e6a8c5597"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "为了快速了解数据的结构，下面的函数将显示数据集中随机选取的一些示例。"
      ],
      "metadata": {
        "id": "Y3vbv6yHWdB8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "from datasets import ClassLabel\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=10):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "    \n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "    display(HTML(df.to_html()))"
      ],
      "outputs": [],
      "metadata": {
        "id": "II9ha_LmWdB9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "show_random_elements(datasets[\"train\"])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Plum cakes made with fresh plums came with other migrants from other traditions in which plum cake is prepared using plum as a primary ingredient . In some versions , the plums may become jam @-@ like inside the cake after cooking , or be prepared using plum jam . Plum cake prepared with plums is also a part of Ashkenazi Jewish cuisine , and is referred to as Pflaumenkuchen or Zwetschgenkuchen . Other plum @-@ based cakes are found in French , Italian and Polish cooking . \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>= = = Language = = = \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The town 's population not only recovered but grew ; the 1906 census of the Canadian Prairies listed the population at 1 @,@ 178 . A new study commissioned by the Dominion government determined that the cracks in the mountain continued to grow and that the risk of another slide remained . Consequently , parts of Frank closest to the mountain were dismantled or relocated to safer areas . \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>The Litigators is a 2011 legal thriller novel by John Grisham , his 25th fiction novel overall . The Litigators is about a two @-@ partner Chicago law firm attempting to strike it rich in a class action lawsuit over a cholesterol reduction drug by a major pharmaceutical drug company . The protagonist is a Harvard Law School grad big law firm burnout who stumbles upon the boutique and joins it only to find himself litigating against his old law firm in this case . The book is regarded as more humorous than most of Grisham 's prior novels . \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>On December 7 , 2006 , Headquarters Marine Corps released a message stating that 2nd Battalion 9th Marines would be reactivated during 2007 as part of the continuing Global War on Terror . 2nd Battalion 9th Marines was re @-@ activated on July 13 , 2007 and replaced the Anti @-@ Terrorism Battalion ( ATBn ) . In September 2008 , Marines and Sailors from 2 / 9 deployed to Al Anbar Province in support of Operation Iraqi Freedom . They were based in the city of Ramadi and returned in April 2009 without any Marines or Sailors killed in action . July 2010 Marines and Sailors from 2 / 9 deployed to Marjah , Helmand Province , Afghanistan in support of Operation Enduring Freedom . In December 2010 Echo Company from 2 / 9 were attached to 3 / 5 in Sangin , Afghanistan where they earned the notorious nickname of \" Green Hats . \" They returned February 2011 . They redeployed back to Marjah December 2011 and returned July 2012 . Echo and Weapons companies deployed once more to Afghanistan from January through April 2013 , participating in combat operations out of Camp Leatherneck . On April 1 , 2015 the battalion was deactivated in a ceremony at Camp Lejeune . \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>( i ) = Indoor \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "LaCaYQyJWdB9",
        "outputId": "8fcf2a87-fa7c-46b1-bd03-26325ce69da9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "正如我们所看到的，一些文本是维基百科文章的完整段落，而其他的只是标题或空行。"
      ],
      "metadata": {
        "id": "LH0Uk_OsWdB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 因果语言模型（Causal Language Modeling，CLM）"
      ],
      "metadata": {
        "id": "9Nu5lPu8WdB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "对于因果语言模型(CLM)，我们首先获取到数据集中的所有文本，并在它们被分词后将它们连接起来。然后，我们将在特定序列长度的例子中拆分它们。通过这种方式，模型将接收如下的连续文本块:\n",
        "\n",
        "```\n",
        "文本1\n",
        "```\n",
        "或\n",
        "```\n",
        "文本1结尾 [BOS_TOKEN] 文本2开头\n",
        "```\n",
        "\n",
        "取决于它们是否跨越数据集中的几个原始文本。标签将与输入相同，但向左移动。\n",
        "\n",
        "在本例中，我们将使用[`distilgpt2`](https://huggingface.co/distilgpt2) 模型。您同样也可以选择[这里](https://huggingface.co/models?filter=causal-lm)列出的任何一个checkpoint:"
      ],
      "metadata": {
        "id": "v7gOchUNWdB-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "model_checkpoint = \"distilgpt2\""
      ],
      "outputs": [],
      "metadata": {
        "id": "z37txOiBWdB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "为了用训练模型时使用的词汇对所有文本进行标记，我们必须下载一个预先训练过的分词器（Tokenizer）。而这些操作都可以由AutoTokenizer类完成:"
      ],
      "metadata": {
        "id": "mk8BWvYWWdB-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "from transformers import AutoTokenizer\n",
        "    \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|██████████| 762/762 [00:00<00:00, 358kB/s]\n",
            "Downloading: 100%|██████████| 1.04M/1.04M [00:04<00:00, 235kB/s]\n",
            "Downloading: 100%|██████████| 456k/456k [00:02<00:00, 217kB/s]\n",
            "Downloading: 100%|██████████| 1.36M/1.36M [00:05<00:00, 252kB/s]\n"
          ]
        }
      ],
      "metadata": {
        "id": "mQwZ5UssWdB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们现在可以对所有的文本调用分词器，该操作可以简单地使用来自Datasets库的map方法实现。首先，我们定义一个在文本上调用标记器的函数:"
      ],
      "metadata": {
        "id": "hAQJGvMxWdB_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"])"
      ],
      "outputs": [],
      "metadata": {
        "id": "wxhKKMYgWdB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "然后我们将它应用到datasets对象中的分词，使用```batch=True```和```4```个进程来加速预处理。而之后我们并不需要```text```列，所以将其舍弃。\n"
      ],
      "metadata": {
        "id": "FM_kMpbCWdB_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "#0:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "#3: 100%|██████████| 2/2 [00:00<00:00,  6.42ba/s]\n",
            "#1: 100%|██████████| 2/2 [00:00<00:00,  5.87ba/s]\n",
            "#0: 100%|██████████| 2/2 [00:00<00:00,  5.56ba/s]\n",
            "\n",
            "#2: 100%|██████████| 2/2 [00:00<00:00,  4.73ba/s]\n",
            "#0:   0%|          | 0/10 [00:00<?, ?ba/s]\n",
            "\u001b[A\n",
            "\n",
            "#0:  10%|█         | 1/10 [00:00<00:03,  2.87ba/s]\n",
            "\u001b[A\n",
            "\n",
            "#0:  20%|██        | 2/10 [00:00<00:02,  2.89ba/s]\n",
            "\u001b[A\n",
            "\n",
            "#0:  30%|███       | 3/10 [00:00<00:02,  3.08ba/s]\n",
            "\u001b[A\n",
            "\n",
            "#0:  40%|████      | 4/10 [00:01<00:01,  3.14ba/s]\n",
            "\u001b[A\n",
            "\n",
            "#0:  50%|█████     | 5/10 [00:01<00:01,  3.33ba/s]\n",
            "\u001b[A\n",
            "\n",
            "#0:  60%|██████    | 6/10 [00:01<00:01,  3.44ba/s]\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "#0:  70%|███████   | 7/10 [00:02<00:01,  2.89ba/s]\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "#0:  80%|████████  | 8/10 [00:02<00:00,  2.89ba/s]\n",
            "\n",
            "#0:  90%|█████████ | 9/10 [00:02<00:00,  3.04ba/s]\n",
            "#0: 100%|██████████| 10/10 [00:02<00:00,  3.37ba/s]\n",
            "#2: 100%|██████████| 10/10 [00:02<00:00,  3.44ba/s]\n",
            "#1: 100%|██████████| 10/10 [00:02<00:00,  3.33ba/s]\n",
            "\n",
            "\n",
            "#3: 100%|██████████| 10/10 [00:03<00:00,  3.25ba/s]\n",
            "#0:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
            "\u001b[A\n",
            "\n",
            "#0: 100%|██████████| 1/1 [00:00<00:00,  3.70ba/s]\n",
            "#1: 100%|██████████| 1/1 [00:00<00:00,  2.79ba/s]\n",
            "\n",
            "\u001b[A\n",
            "\n",
            "#2: 100%|██████████| 1/1 [00:00<00:00,  2.74ba/s]\n",
            "#3: 100%|██████████| 1/1 [00:00<00:00,  2.82ba/s]\n"
          ]
        }
      ],
      "metadata": {
        "id": "rNb1U12YWdCA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "如果我们现在查看数据集的一个元素，我们会看到文本已经被模型所需的input_ids所取代:"
      ],
      "metadata": {
        "id": "bc2niRZJWdCA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "tokenized_datasets[\"train\"][1]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " 'input_ids': [796, 569, 18354, 7496, 17740, 6711, 796, 220, 198]}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgC4UWv8WdCA",
        "outputId": "e3257089-88b6-4b15-fbe1-45073073ad3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "下一步就有点小困难了：我们需要将所有文本连接在一起，然后将结果分割成特定`block_size`的小块。为此，我们将再次使用`map`方法，并使用选项`batch=True`。这个选项允许我们通过返回不同数量的样本来改变数据集中的样本数量。通过这种方式，我们可以从一批示例中创建新的示例。\n",
        "\n",
        "首先，我们需要获取预训练模型时所使用的最大长度。最大长度在这里设置为128，以防您的显存爆炸💥。"
      ],
      "metadata": {
        "id": "TpKE1TJXWdCA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "# block_size = tokenizer.model_max_length\n",
        "block_size = 128"
      ],
      "outputs": [],
      "metadata": {
        "id": "uEnLI7LJWdCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "然后我们编写预处理函数来对我们的文本进行分组:"
      ],
      "metadata": {
        "id": "IwZu_FSjWdCB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "def group_texts(examples):\n",
        "    # 拼接所有文本\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # 我们将余数对应的部分去掉。但如果模型支持的话，可以添加padding，您可以根据需要定制此部件。\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "    # 通过max_len进行分割。\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ],
      "outputs": [],
      "metadata": {
        "id": "OhhL0v2FWdCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "首先注意，我们复制了标签的输入。\n",
        "\n",
        "这是因为🤗transformer库的模型默认向右移动，所以我们不需要手动操作。\n",
        "\n",
        "还要注意，在默认情况下，`map`方法将发送一批1,000个示例，由预处理函数处理。因此，在这里，我们将删除剩余部分，使连接的标记化文本每1000个示例为`block_size`的倍数。您可以通过传递更高的批处理大小来调整此行为(当然这也会被处理得更慢)。你也可以使用`multiprocessing`来加速预处理:"
      ],
      "metadata": {
        "id": "DpsALat7WdCC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "lm_datasets = tokenized_datasets.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    batch_size=1000,\n",
        "    num_proc=4,\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "#0:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "#3: 100%|██████████| 2/2 [00:00<00:00,  6.12ba/s]\n",
            "#1: 100%|██████████| 2/2 [00:00<00:00,  4.89ba/s]\n",
            "#0: 100%|██████████| 2/2 [00:00<00:00,  4.60ba/s]\n",
            "\n",
            "#2: 100%|██████████| 2/2 [00:00<00:00,  3.94ba/s]\n",
            "#0:   0%|          | 0/10 [00:00<?, ?ba/s]\n",
            "\u001b[A\n",
            "\n",
            "#0:  10%|█         | 1/10 [00:00<00:03,  2.90ba/s]\n",
            "\u001b[A\n",
            "\n",
            "#0:  20%|██        | 2/10 [00:00<00:02,  2.76ba/s]\n",
            "\u001b[A\n",
            "\n",
            "#0:  30%|███       | 3/10 [00:01<00:02,  2.72ba/s]\n",
            "\u001b[A\n",
            "\n",
            "#0:  40%|████      | 4/10 [00:01<00:02,  2.75ba/s]\n",
            "\u001b[A\n",
            "\n",
            "#0:  50%|█████     | 5/10 [00:01<00:01,  2.92ba/s]\n",
            "#0:  60%|██████    | 6/10 [00:02<00:01,  3.01ba/s]\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "#0:  70%|███████   | 7/10 [00:02<00:01,  2.69ba/s]\n",
            "\n",
            "#0:  80%|████████  | 8/10 [00:02<00:00,  2.67ba/s]\n",
            "\u001b[A\n",
            "\n",
            "#0: 100%|██████████| 10/10 [00:03<00:00,  3.00ba/s]\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "#2: 100%|██████████| 10/10 [00:03<00:00,  3.04ba/s]\n",
            "#1: 100%|██████████| 10/10 [00:03<00:00,  2.88ba/s]\n",
            "\n",
            "\n",
            "#3: 100%|██████████| 10/10 [00:03<00:00,  2.79ba/s]\n",
            "#0:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
            "\u001b[A\n",
            "\n",
            "#0: 100%|██████████| 1/1 [00:00<00:00,  3.41ba/s]\n",
            "#1: 100%|██████████| 1/1 [00:00<00:00,  2.61ba/s]\n",
            "\n",
            "\n",
            "#3: 100%|██████████| 1/1 [00:00<00:00,  2.69ba/s]\n",
            "\n",
            "#2: 100%|██████████| 1/1 [00:00<00:00,  2.55ba/s]\n"
          ]
        }
      ],
      "metadata": {
        "id": "lmoi9YUZWdCC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "现在我们可以检查数据集是否发生了变化：现在样本包含了`block_size`连续字符块，可能跨越了几个原始文本。"
      ],
      "metadata": {
        "id": "qPkZvXnCWdCD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' game and follows the \" Nameless \", a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \". \\n The game began development in 2010, carrying over a large portion of the work done on Valkyria Chronicles II. While it retained the standard features of the series, it also underwent multiple adjustments, such as making the game more forgiving for series newcomers. Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries, along with Valkyria Chronicles II director Takeshi Oz'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "49I25iXJWdCD",
        "outputId": "f7f0364e-7ac0-44e0-aed1-d483b1dda631"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "既然数据已经清理完毕，我们就可以实例化我们的训练器了。我们将建立一个模型:"
      ],
      "metadata": {
        "id": "qVkDCaz5WdCD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|██████████| 353M/353M [00:21<00:00, 16.0MB/s]\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "f9a94ec0a95a435b8f58cc67994099f7",
            "782ab18684dd4f36a403d180138e8f1d",
            "6d8e4de69163477891b6636d869f6e4e",
            "27a5fef432d24131b2678f9cf5906a4f",
            "36d6db8a50de4a3c886647f31b60b621",
            "969eb6b77105455cab015cb8574b1bd3",
            "a0d0fe9dec9b413fa80fa4d678dcb9c3",
            "7ecafb32516947aa9ace74da667d9665"
          ]
        },
        "id": "jm5DOjPOWdCD",
        "outputId": "5ec1e6e5-66ef-4033-fdc4-3ee3ee6e0dd9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "检查torch版本"
      ],
      "metadata": {
        "id": "sAFO2Y6_WdCE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "\n",
        "import importlib.util\n",
        "# import importlib_metadata\n",
        "a = importlib.util.find_spec(\"torch\") is not None\n",
        "print(a)\n",
        "# _torch_version = importlib_metadata.version(\"torch\")\n",
        "# print(_torch_version)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11XlW2ogWdCE",
        "outputId": "d8e8c62e-dfc0-43d1-b377-2b1796a52f56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "和一些`TrainingArguments`:"
      ],
      "metadata": {
        "id": "y8_qK5i3WdCE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "source": [
        "from transformers import Trainer, TrainingArguments"
      ],
      "outputs": [],
      "metadata": {
        "id": "WWhPVy82WdCE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "training_args = TrainingArguments(\n",
        "    \"test-clm\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "id": "qGz6BxoOWdCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们把这些都传递给`Trainer`类:"
      ],
      "metadata": {
        "id": "efdl12AIWdCF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_datasets[\"train\"][:1000],\n",
        "    eval_dataset=lm_datasets[\"validation\"][:1000],\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "id": "1ukmE65zWdCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "然后就可以训练我们的模型🌶:"
      ],
      "metadata": {
        "id": "UDn2o1gSWdCF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "source": [
        "trainer.train()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/2k/x3py0v857kgcwqvvl00xxhxw0000gn/T/ipykernel_12460/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/Desktop/zhihu/learn-nlp-with-transformers/venv/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                 \u001b[0;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Desktop/zhihu/learn-nlp-with-transformers/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Desktop/zhihu/learn-nlp-with-transformers/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Desktop/zhihu/learn-nlp-with-transformers/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Desktop/zhihu/learn-nlp-with-transformers/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 1"
          ]
        }
      ],
      "metadata": {
        "id": "a55CO2xGWdCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "一旦训练完成，我们就可以评估我们的模型，得到它在验证集上的perplexity，如下所示:"
      ],
      "metadata": {
        "id": "PAFX3mCwWdCG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import math\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "g1A7eBP3WdCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 掩蔽语言模型（Mask Language Modeling，MLM）"
      ],
      "metadata": {
        "id": "lJzcfmsVWdCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "掩蔽语言模型(MLM)我们将使用相同的数据集预处理和以前一样用一个额外的步骤：\n",
        "\n",
        "我们将随机\"MASK\"一些字符(使用\"[MASK]\"进行替换)以及调整标签为只包含在\"[MASK]\"位置处的标签(因为我们不需要预测没有被\"MASK\"的字符)。\n",
        "\n",
        "在本例中，我们将使用[`distilroberta-base`](https://huggingface.co/distilroberta-base)模型。您同样也可以选择[这里](https://huggingface.co/models?filter=causal-lm)列出的任何一个checkpoint:"
      ],
      "metadata": {
        "id": "sr5UHTPjWdCG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model_checkpoint = \"distilroberta-base\""
      ],
      "outputs": [],
      "metadata": {
        "id": "2X4qJPeXWdCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们可以像之前一样应用相同的分词器函数，我们只需要更新我们的分词器来使用刚刚选择的checkpoint:"
      ],
      "metadata": {
        "id": "GMDHywzqWdCH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
        "tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
      ],
      "outputs": [],
      "metadata": {
        "id": "sgIqOa4uWdCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "像之前一样，我们把文本分组在一起，并把它们分成长度为`block_size`的样本。如果您的数据集由单独的句子组成，则可以跳过这一步。"
      ],
      "metadata": {
        "id": "O4BALsgJWdCH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "lm_datasets = tokenized_datasets.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    batch_size=1000,\n",
        "    num_proc=4,\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "id": "d4jJo5X2WdCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "剩下的和我们之前的做法非常相似，只有两个例外。首先我们使用一个适合掩蔽语言模型的模型:"
      ],
      "metadata": {
        "id": "2wbjmPZQWdCI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from transformers import AutoModelForMaskedLM\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
      ],
      "outputs": [],
      "metadata": {
        "id": "LkJuOG4oWdCI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "其次，我们使用一个特殊的data_collator。data_collator是一个函数，负责获取样本并将它们批处理成张量。\n",
        "\n",
        "在前面的例子中，我们没有什么特殊的事情要做，所以我们只使用这个参数的默认值。这里我们要做随机\"MASK\"。\n",
        "\n",
        "我们可以将其作为预处理步骤(`tokenizer`)进行处理，但在每个阶段，字符总是以相同的方式被掩盖。通过在data_collator中执行这一步，我们可以确保每次检查数据时都以新的方式完成随机掩蔽。\n",
        "\n",
        "为了实现掩蔽，`Transformers`为掩蔽语言模型提供了一个`DataCollatorForLanguageModeling`。我们可以调整掩蔽的概率:"
      ],
      "metadata": {
        "id": "MVKOscEdWdCI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
      ],
      "outputs": [],
      "metadata": {
        "id": "A-k8wJK7WdCI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "然后我们要把所有的东西交给trainer，然后开始训练:"
      ],
      "metadata": {
        "id": "m83JcPGyWdCI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_datasets[\"train\"][:1000],\n",
        "    eval_dataset=lm_datasets[\"validation\"][:100],\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "id": "I12S2ZQxWdCJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "trainer.train()"
      ],
      "outputs": [],
      "metadata": {
        "id": "u_4PHx1CWdCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "像以前一样，我们可以在验证集上评估我们的模型。\n",
        "\n",
        "与CLM目标相比，困惑度要低得多，因为对于MLM目标，我们只需要对隐藏的令牌(在这里占总数的15%)进行预测，同时可以访问其余的令牌。\n",
        "\n",
        "因此，对于模型来说，这是一项更容易的任务。"
      ],
      "metadata": {
        "id": "MDKbrOmzWdCJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "eval_results = trainer.evaluate()\n",
        "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "60hUa-W5WdCJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "不要忘记将你的模型[上传](https://huggingface.co/transformers/model_sharing.html)到[🤗 模型中心](https://huggingface.co/models)。"
      ],
      "outputs": [],
      "metadata": {
        "id": "uPa5UTWWWdCK"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "4.5-生成任务-语言模型",
      "provenance": []
    },
    "interpreter": {
      "hash": "3bfce0b4c492a35815b5705a19fe374a7eea0baaa08b34d90450caf1fe9ce20b"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('venv': virtualenv)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "27a5fef432d24131b2678f9cf5906a4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ecafb32516947aa9ace74da667d9665",
            "placeholder": "​",
            "style": "IPY_MODEL_a0d0fe9dec9b413fa80fa4d678dcb9c3",
            "value": " 353M/353M [00:11&lt;00:00, 31.6MB/s]"
          }
        },
        "36d6db8a50de4a3c886647f31b60b621": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "6d8e4de69163477891b6636d869f6e4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_969eb6b77105455cab015cb8574b1bd3",
            "max": 352833716,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36d6db8a50de4a3c886647f31b60b621",
            "value": 352833716
          }
        },
        "782ab18684dd4f36a403d180138e8f1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ecafb32516947aa9ace74da667d9665": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "969eb6b77105455cab015cb8574b1bd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0d0fe9dec9b413fa80fa4d678dcb9c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9a94ec0a95a435b8f58cc67994099f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d8e4de69163477891b6636d869f6e4e",
              "IPY_MODEL_27a5fef432d24131b2678f9cf5906a4f"
            ],
            "layout": "IPY_MODEL_782ab18684dd4f36a403d180138e8f1d"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}