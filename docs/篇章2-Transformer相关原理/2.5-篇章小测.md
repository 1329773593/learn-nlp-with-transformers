## 篇章小测
* 问题1: Transformer中的softmax计算为什么需要除以$d_k$?
* 问题2: Transformer中attention score计算时候如何mask掉padding位置？
* 问题3: 为什么Transformer中加入了positional embedding？
* 问题4: BERT预训练时mask的比例，可以mask更大的比例吗？
* 问题5: BERT如何进行tokenize操作？有什么好处？
* 问题6: GPT如何进行tokenize操作？和BERT的区别是什么？
* 问题7: BERT模型特别大，单张GPU训练仅仅只能放入1个batch的时候，怎么训练？